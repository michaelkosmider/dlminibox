{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dl import Module # Always import this if you wish to build your own Module using existing Modules. \n",
    "\n",
    "from dl.modules import Linear, ReLU # Typical modules found in a multilayer perceptron.\n",
    "\n",
    "from dl.optimizers import SGD # We need this to update model weights.\n",
    "\n",
    "from dl.data import BatchLoader, train_val_split, accuracy # We will pass in in our training data into a batchloader, similar to the PyTorch DataLoader.\n",
    "from dl.data.transforms import ToVariable, ToFloat, Normalize, ComposeTransforms # The transformations done to raw numpy image and label instances, before being passed into the model.\n",
    "\n",
    "from dl.functions import cross_entropy_loss # We need this to evaluate the quality of our current model weights with respect to a batch.\n",
    "\n",
    "from dl import Variable # The datatype used by this framework, which wraps NumPy arrays. It is within Variables, and specifically the .node attribute, that autograd takes place. \n",
    "\n",
    "# For vizualizing output\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For downloading MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a toy dataset (MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (70000, 784)\n",
      "Labels shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Optional: Set a data cache directory\n",
    "data_home = os.path.join(\"data\", \"mnist\")\n",
    "\n",
    "# Download and cache MNIST (70000 samples)\n",
    "mnist = fetch_openml('mnist_784', version=1, data_home=data_home, as_frame=False)\n",
    "\n",
    "# Access the data\n",
    "X, y = mnist.data, mnist.target  # X is (70000, 784), y is (70000,)\n",
    "y = y.astype(int)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "X_train_full, y_train_full = X[:60000], y[:60000]\n",
    "X_train, y_train, X_val, y_val = train_val_split(X_train_full, y_train_full, 0.1)\n",
    "\n",
    "X_test, y_test = X[60000:], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transforms (division by 255.0 and normalization)\n",
    "\n",
    "# Compute normalization statistics.\n",
    "X_for_stats = X_train_full.astype(np.float32) / 255.0\n",
    "mean = np.mean(X_for_stats)\n",
    "std = np.std(X_for_stats)\n",
    "\n",
    "image_transforms = ComposeTransforms([ToFloat(255.0), Normalize(mean, std), ToVariable()])\n",
    "label_transforms = ToVariable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and batchloaders.\n",
    "\n",
    "class MNIST:\n",
    "    \n",
    "    def __init__(self, images, labels, image_transforms=None, label_transforms=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.image_transforms = image_transforms\n",
    "        self.label_transforms = label_transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.image_transforms is not None:\n",
    "            image = self.image_transforms(image)\n",
    "        \n",
    "        if self.label_transforms is not None:\n",
    "            label = self.label_transforms(label)\n",
    "            \n",
    "        return image, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "BATCH_SIZE = 256\n",
    "    \n",
    "batchloaders = {}\n",
    "batchloaders['train'] = BatchLoader(MNIST(X_train, y_train, image_transforms, label_transforms), batch_size=BATCH_SIZE, shuffle=True)\n",
    "batchloaders['val'] = BatchLoader(MNIST(X_val, y_val, image_transforms, label_transforms), batch_size=BATCH_SIZE, shuffle=False)\n",
    "batchloaders['test'] = BatchLoader(MNIST(X_test, y_test, image_transforms, label_transforms), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a toy model.\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assign your submodules here as attributes.\n",
    "        self.lin1 = Linear(784, 128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.lin2 = Linear(128, 64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.lin3 = Linear(64, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Use your submodules to compute something for forward. Autograd happens here.\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.lin2(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.lin3(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "    lin1 : Linear(input_size = 784, output_size = 128, param_init = 'xavier')\n",
      "    relu1 : ReLU\n",
      "    lin2 : Linear(input_size = 128, output_size = 64, param_init = 'xavier')\n",
      "    relu2 : ReLU\n",
      "    lin3 : Linear(input_size = 64, output_size = 10, param_init = 'xavier')\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "model.print() # There is a print method to view your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = SGD(model.parameters(), learning_rate=0.01, weight_decay=1e-4 , momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4316 | Val Loss: 0.2077\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1843 | Val Loss: 0.1611\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1386 | Val Loss: 0.1349\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1116 | Val Loss: 0.1100\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0927 | Val Loss: 0.1058\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0788 | Val Loss: 0.0996\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0683 | Val Loss: 0.0940\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0607 | Val Loss: 0.0895\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0534 | Val Loss: 0.0911\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0468 | Val Loss: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.enable_grad()\n",
    "    pbar = tqdm(batchloaders['train'], desc=\"Training\", leave=False)\n",
    "    for X_batch, y_batch in pbar:\n",
    "        \n",
    "        # Compute features and loss.\n",
    "        features = model(X_batch)\n",
    "        loss = cross_entropy_loss(features, y_batch)\n",
    "       \n",
    "        # Update model parameters.\n",
    "        optimizer.clear_grad()\n",
    "        loss.backward()\n",
    "        optimizer.update_parameters()\n",
    "    \n",
    "        train_loss += loss.data * len(y_batch.data)\n",
    "        train_correct += accuracy(features, y_batch)\n",
    "        train_total += len(y_batch.data)\n",
    "        \n",
    "        # Update tqdm bar with batch loss\n",
    "        pbar.set_postfix(loss=loss.data)\n",
    "        \n",
    "    train_losses.append(train_loss / train_total)\n",
    "    train_accuracies.append(train_correct / train_total)\n",
    "    \n",
    "    # Validate\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.disable_grad()\n",
    "    for X_batch, y_batch in tqdm(batchloaders['val'], desc=\"Validating\", leave=False):\n",
    "        \n",
    "        features = model(X_batch)\n",
    "        loss = cross_entropy_loss(features, y_batch)\n",
    "        \n",
    "        val_loss += loss.data * len(y_batch.data)\n",
    "        val_correct += accuracy(features, y_batch)\n",
    "        val_total += len(y_batch.data)\n",
    "        \n",
    "    val_losses.append(val_loss / val_total)\n",
    "    val_accuracies.append(val_correct / val_total)\n",
    "    print(f\"Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0819 | Test Accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "model.disable_grad()\n",
    "for X_batch, y_batch in batchloaders['test']:\n",
    "    \n",
    "    features = model(X_batch)\n",
    "    loss = cross_entropy_loss(features, y_batch)\n",
    "    test_loss += loss.data * len(y_batch.data)\n",
    "    test_correct += accuracy(features, y_batch)\n",
    "    test_total += len(y_batch.data)\n",
    "    \n",
    "print(f\"Test Loss: {(test_loss / test_total):.4f} | Test Accuracy: {(test_correct / test_total):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlminibox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
