{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a toy dataset (MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data directly from csv.\n",
    "\n",
    "mnist_train = np.genfromtxt('./data/mnist_train.csv', delimiter=',')\n",
    "mnist_test = np.genfromtxt('./data/mnist_test.csv', delimiter=',')\n",
    "\n",
    "# Splitting the data into features and labels.\n",
    "\n",
    "X_train = mnist_train[1:,1:]\n",
    "y_train = mnist_train[1:,0].astype(int)\n",
    "\n",
    "X_test = mnist_test[1:,1:]\n",
    "y_test = mnist_test[1:,0].astype(int)\n",
    "\n",
    "# Splitting off a part of training set for validation.\n",
    "\n",
    "N_train = y_train.shape[0]\n",
    "order = np.random.permutation(N_train)\n",
    "split_index = int(N_train*0.8)\n",
    "\n",
    "\n",
    "X_valid = X_train[order][split_index:]\n",
    "X_train = X_train[order][:split_index]\n",
    "y_valid = y_train[order][split_index:]\n",
    "y_train = y_train[order][:split_index]\n",
    "\n",
    "# Squeezing the data into [0,1], which improves training.\n",
    "\n",
    "X_train /= 255\n",
    "X_valid /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl import Module # Always import this if you wish to build your own Module using existing Modules. \n",
    "from dl.modules import Linear, ReLU # Typical modules in a multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a toy model.\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assign your submodules here as attributes.\n",
    "        self.lin1 = Linear(784, 128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.lin2 = Linear(128, 64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.lin3 = Linear(64, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Use your submodules to compute something for forward. Autograd happens here.\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.lin2(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.lin3(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "    lin1 : Linear(input_size = 784, output_size = 128, param_init = 'xavier')\n",
      "    relu1 : ReLU\n",
      "    lin2 : Linear(input_size = 128, output_size = 64, param_init = 'xavier')\n",
      "    relu2 : ReLU\n",
      "    lin3 : Linear(input_size = 64, output_size = 10, param_init = 'xavier')\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "model.print() # There is a print method to view your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl.optimizers import SGD # We need this to update model weights, aka train the model.\n",
    "optimizer = SGD(model.parameters(), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl import Variable # The datatype passed into any Module or Function must be a Variable. It is within Variables, and specifically the .node attribute, that autograd takes place. \n",
    "\n",
    "X_train = Variable(X_train)\n",
    "X_valid = Variable(X_valid)\n",
    "X_test = Variable(X_test)\n",
    "\n",
    "y_train = Variable(y_train)\n",
    "y_valid = Variable(y_valid)\n",
    "y_test = Variable(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 72.14921462103425\n",
      "Epoch loss: 39.696954810601106\n",
      "Epoch loss: 24.77514213025257\n",
      "Epoch loss: 19.35460897137209\n",
      "Epoch loss: 16.75968445122756\n"
     ]
    }
   ],
   "source": [
    "from dl.data.iterate import iterate_batches # Passing in our training data into iterate_batches gives us the batches we need for each loss and gradient calculation.\n",
    "from dl.functions import cross_entropy_loss # We need this to evaluate the quality of our current model weights with respect to a batch.\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in iterate_batches(X_train, y_train, batch_size=256, shuffle=True):\n",
    "\n",
    "        features = model(X_batch)\n",
    "        loss = cross_entropy_loss(features, y_batch)\n",
    "        loss.backward() # Calculate the gradient (aka, the direction in which to change model weights in order to lower the current loss, as suggested by this immediate batch).\n",
    "\n",
    "        epoch_loss += loss.data\n",
    "\n",
    "        optimizer.update_parameters()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "\n",
    "    print(\"Epoch loss:\", epoch_loss/epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
