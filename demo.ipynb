{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dl import Module # Always import this if you wish to build your own Module using existing Modules. \n",
    "\n",
    "from dl.modules import Linear, ReLU # Typical modules found in a multilayer perceptron.\n",
    "\n",
    "from dl.optimizers import SGD # We need this to update model weights.\n",
    "\n",
    "from dl.data import BatchLoader, train_val_split # We will pass in in our training data into a batchloader, similar to the PyTorch DataLoader.\n",
    "from dl.data.transforms import ToVariable, ToFloat, Normalize, ComposeTransforms # The transformations done to raw numpy image and label instances, before being passed into the model.\n",
    "\n",
    "from dl.functions import cross_entropy_loss # We need this to evaluate the quality of our current model weights with respect to a batch.\n",
    "\n",
    "from dl import Variable # The datatype used by this framework, which wraps NumPy arrays. It is within Variables, and specifically the .node attribute, that autograd takes place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a toy dataset (MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data directly from csv.\n",
    "\n",
    "mnist_train = np.genfromtxt('./data/mnist_train.csv', delimiter=',')\n",
    "mnist_test = np.genfromtxt('./data/mnist_test.csv', delimiter=',')\n",
    "\n",
    "# Splitting the data into features and labels.\n",
    "\n",
    "X_train_full = mnist_train[1:,1:]\n",
    "y_train_full = mnist_train[1:,0].astype(int)\n",
    "\n",
    "X_test = mnist_test[1:,1:]\n",
    "y_test = mnist_test[1:,0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalization statistics.\n",
    "X_for_stats = X_train_full.astype(np.float32) / 255.0\n",
    "mean = np.mean(X_for_stats)\n",
    "std = np.std(X_for_stats)\n",
    "\n",
    "\n",
    "image_transforms = ComposeTransforms([ToFloat(), Normalize(mean, std), ToVariable()])\n",
    "label_transforms = ToVariable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(48000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNIST:\n",
    "    \n",
    "    def __init__(self, images, labels, image_transforms=None, label_transforms=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.image_transforms = image_transforms\n",
    "        self.label_transforms = label_transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.image_transforms is not None:\n",
    "            image = self.image_transforms(image)\n",
    "        \n",
    "        if self.label_transforms is not None:\n",
    "            label = self.label_transforms(label)\n",
    "            \n",
    "        return image, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a toy model.\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assign your submodules here as attributes.\n",
    "        self.lin1 = Linear(784, 128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.lin2 = Linear(128, 64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.lin3 = Linear(64, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Use your submodules to compute something for forward. Autograd happens here.\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.lin2(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.lin3(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "    lin1 : Linear(input_size = 784, output_size = 128, param_init = 'xavier')\n",
      "    relu1 : ReLU\n",
      "    lin2 : Linear(input_size = 128, output_size = 64, param_init = 'xavier')\n",
      "    relu2 : ReLU\n",
      "    lin3 : Linear(input_size = 64, output_size = 10, param_init = 'xavier')\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "model.print() # There is a print method to view your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 1.760192656193778\n",
      "Epoch loss: 0.9028898164895317\n",
      "Epoch loss: 0.6118096185742777\n",
      "Epoch loss: 0.5001693849202595\n",
      "Epoch loss: 0.44010433171269275\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    iterations = 0\n",
    "    for X_batch, y_batch in iterate_batches(X_train, y_train, batch_size=256, shuffle=True):\n",
    "\n",
    "        # Compute features and loss.\n",
    "        features = model(X_batch)\n",
    "        loss = cross_entropy_loss(features, y_batch)\n",
    "\n",
    "        # Update model parameters.\n",
    "        optimizer.update_parameters()\n",
    "        loss.backward() # Calculate the gradient (aka, the direction in which to change model weights in order to lower the current loss, as suggested by this immediate batch).\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        epoch_loss += loss.data\n",
    "        iterations += 1\n",
    "\n",
    "\n",
    "    print(\"Epoch loss:\", epoch_loss/iterations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
