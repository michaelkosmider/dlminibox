{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "379979d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dl import Variable, Module\n",
    "import numpy as np\n",
    "from dl.functions import sum, cross_entropy_loss\n",
    "from dl.modules import Convolution, Linear, ReLU, Flatten, MaxPool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61af5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights_to_torch(model, torch_model):\n",
    "    torch_params = [p for p in torch_model.parameters()]\n",
    "    params = model.parameters()\n",
    "    \n",
    "    assert len(torch_params) == len(params), \"Parameter count mismatch\"\n",
    "\n",
    "    for torch_p, p in zip(torch_params, params):\n",
    "        p = torch.tensor(p.data, dtype=torch.float32)\n",
    "        if p.shape != torch_p.data.shape:\n",
    "            p = p.T\n",
    "\n",
    "        torch_p.data.copy_(p)\n",
    "\n",
    "def compare_grads(model, torch_model):\n",
    "    torch_params = [p for p in torch_model.parameters()]\n",
    "    params = model.parameters()\n",
    "    \n",
    "    assert len(torch_params) == len(params), \"Parameter count mismatch\"\n",
    "\n",
    "    for torch_p, p in zip(torch_params, params):\n",
    "\n",
    "        # My framework stores W in Linear layer as the transpose so that the output is Y = X @ W + b.\n",
    "        # This workaround fails for square weights, so come up with a better solution in the future.\n",
    "        mygrad = p.grad()\n",
    "        if p.grad().shape != torch_p.grad.shape:\n",
    "            mygrad = mygrad.T\n",
    "\n",
    "        print(np.allclose(torch_p.grad.numpy(), mygrad, atol=1e-4, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49183ec",
   "metadata": {},
   "source": [
    "## Convolution Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23da9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# def compare_grads(C_in, C_out, K, stride, padding, N_batch, H, W, verbose=False):\n",
    "#     # Input\n",
    "#     X = Variable(np.random.randn(N_batch, C_in, H, W), keep_grad=True)\n",
    "#     X_torch = torch.tensor(X.data, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#     # Your Conv\n",
    "#     conv = Convolution(C_in, C_out, K, stride=stride, padding=padding)\n",
    "#     Y = conv(X)\n",
    "#     z = sum(Y)\n",
    "#     z.backward()\n",
    "\n",
    "#     # PyTorch Conv\n",
    "#     conv_torch = nn.Conv2d(C_in, C_out, K, stride=stride, padding=padding, bias=False)\n",
    "#     W_tensor = torch.tensor(conv.W.data, dtype=torch.float32)\n",
    "#     with torch.no_grad():\n",
    "#         conv_torch.weight.copy_(W_tensor)\n",
    "#     Y_torch = conv_torch(X_torch)\n",
    "#     z_torch = torch.sum(Y_torch)\n",
    "#     z_torch.backward()\n",
    "\n",
    "#     grad_torch_W = conv_torch.weight.grad.numpy()\n",
    "#     grad_mine_W = conv.W.grad()\n",
    "\n",
    "#     grad_torch_X = X_torch.grad.numpy()\n",
    "#     grad_mine_X = X.grad()\n",
    "\n",
    "#     w_close = np.allclose(grad_mine_W, grad_torch_W, atol=1e-4, rtol=1e-3)\n",
    "#     x_close = np.allclose(grad_mine_X, grad_torch_X, atol=1e-4, rtol=1e-3)\n",
    "\n",
    "#     if not (w_close and x_close) and verbose:\n",
    "#         print(f\"FAILED: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={padding}, size=({N_batch},{H},{W})\")\n",
    "#         if not w_close:\n",
    "#             print(\"Weight grad mismatch — max diff:\", np.max(np.abs(grad_mine_W - grad_torch_W)))\n",
    "#         if not x_close:\n",
    "#             print(\"Input grad mismatch — max diff:\", np.max(np.abs(grad_mine_X - grad_torch_X)))\n",
    "\n",
    "#     return w_close and x_close\n",
    "\n",
    "# def run_all_tests():\n",
    "#     paddings = [0, 1, 2, 60]\n",
    "#     strides = [1, 2, 37, 90]\n",
    "#     kernels = [1, 3, 5]\n",
    "#     batch_sizes = [1, 4]\n",
    "#     image_sizes = [(5, 5), (7, 7), (16, 16), (8, 32)]\n",
    "#     channels = [(1, 1), (3, 8), (8, 3), (4, 4)]\n",
    "\n",
    "#     total = 0\n",
    "#     passed = 0\n",
    "\n",
    "#     for pad in paddings:\n",
    "#         for stride in strides:\n",
    "#             for K in kernels:\n",
    "#                 for (C_in, C_out) in channels:\n",
    "#                     for N in batch_sizes:\n",
    "#                         for (H, W) in image_sizes:\n",
    "#                             if H + 2 * pad < K or W + 2 * pad < K:\n",
    "#                                 continue  # Skip invalid kernel size\n",
    "#                             total += 1\n",
    "#                             try:\n",
    "#                                 result = compare_grads(C_in, C_out, K, stride, pad, N, H, W)\n",
    "#                             except Exception as e:\n",
    "#                                 print(f\"\\n❌ Exception for: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={pad}, size=({N},{H},{W})\")\n",
    "#                                 raise e\n",
    "#                             if result:\n",
    "#                                 passed += 1\n",
    "#                             else:\n",
    "#                                 print(f\"\\n❌ Failed for: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={pad}, size=({N},{H},{W})\")\n",
    "#                                 return\n",
    "#     print(f\"✅ Passed {passed} / {total} tests.\")\n",
    "\n",
    "# run_all_tests()\n",
    "\n",
    "# def test_edge_cases():\n",
    "#     assert compare_grads(1, 1, 1, 1, 0, 1, 1, 1)  # Minimal case\n",
    "#     assert compare_grads(3, 4, 3, 2, 4, 16, 7, 7)  # From your original test\n",
    "#     assert compare_grads(8, 16, 5, 1, 2, 2, 32, 32)  # Large channels and image\n",
    "#     assert compare_grads(2, 2, 3, 1, 1, 1, 5, 5)  # Small batch\n",
    "\n",
    "# test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f47751",
   "metadata": {},
   "source": [
    "## Small CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86837d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class MiniCNN(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Convolution(3, 4, 4, 1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool(2, 2)\n",
    "        self.flat1 = Flatten()\n",
    "        self.lin1 = Linear(4*14*14, 783)\n",
    "        self.relu2 = ReLU()\n",
    "        self.lin2 = Linear(783, 100)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = self.conv1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.maxpool1(X)\n",
    "        X = self.flat1(X)\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.lin2(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "class MiniCNNTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 4, 4, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(4*14*14, 783)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(783, 100)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.maxpool1(X)\n",
    "        X = self.flat1(X)\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.lin2(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "N = 16\n",
    "C_in = 3\n",
    "H = 32\n",
    "W = 32\n",
    "\n",
    "X = Variable(np.random.rand(N, C_in, H, W))\n",
    "y = Variable(np.random.randint(0,100,size=(N,)))\n",
    "torch_X = torch.tensor(X.data, dtype=torch.float32)\n",
    "torch_y = torch.tensor(y.data, dtype=torch.int64)\n",
    "\n",
    "model = MiniCNN()\n",
    "torch_model = MiniCNNTorch()\n",
    "\n",
    "transfer_weights_to_torch(model, torch_model)\n",
    "\n",
    "Y = model(X)\n",
    "z = cross_entropy_loss(Y, y)\n",
    "z.backward()\n",
    "\n",
    "torch_Y = torch_model(torch_X) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch_z = criterion(torch_Y, torch_y)\n",
    "torch_z.backward()\n",
    "\n",
    "compare_grads(model, torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "580a65f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 0.0899,  0.1697,  0.1450,  0.1167],\n",
       "            [-0.0900,  0.1768,  0.1444,  0.0441],\n",
       "            [ 0.0633, -0.2430, -0.2675,  0.2555],\n",
       "            [ 0.0709, -0.0725, -0.0381,  0.0830]],\n",
       "  \n",
       "           [[-0.1012, -0.1302,  0.1744,  0.1130],\n",
       "            [-0.0906, -0.0400,  0.0317, -0.3291],\n",
       "            [-0.1970,  0.2128, -0.1069,  0.0964],\n",
       "            [ 0.2199, -0.1555, -0.1341,  0.1829]],\n",
       "  \n",
       "           [[ 0.1485,  0.1133, -0.2545,  0.0491],\n",
       "            [-0.2866,  0.1695, -0.0532, -0.1994],\n",
       "            [ 0.0366, -0.2024, -0.0646,  0.1049],\n",
       "            [ 0.1056,  0.2547,  0.0240,  0.2198]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2995, -0.1352, -0.3815, -0.2840],\n",
       "            [-0.2070, -0.1778, -0.1658, -0.2293],\n",
       "            [-0.0700, -0.0230,  0.0055, -0.2034],\n",
       "            [-0.2276, -0.1138,  0.1241,  0.2154]],\n",
       "  \n",
       "           [[ 0.0719,  0.2426,  0.1248,  0.1459],\n",
       "            [ 0.0799,  0.2767, -0.0394, -0.3173],\n",
       "            [ 0.1599, -0.2924,  0.2857,  0.3905],\n",
       "            [ 0.2638, -0.2126,  0.1508,  0.1840]],\n",
       "  \n",
       "           [[-0.3579, -0.1915, -0.3099,  0.2173],\n",
       "            [-0.1386,  0.0901,  0.4316,  0.3079],\n",
       "            [-0.0557,  0.2358,  0.1429, -0.1901],\n",
       "            [-0.0963, -0.2337, -0.1848, -0.2315]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1020, -0.1322,  0.2113, -0.2636],\n",
       "            [-0.1324,  0.1724, -0.2614,  0.0605],\n",
       "            [-0.2351, -0.0739, -0.2047,  0.3617],\n",
       "            [ 0.2479,  0.3462, -0.4284, -0.1313]],\n",
       "  \n",
       "           [[ 0.3529, -0.1268, -0.1000,  0.1904],\n",
       "            [ 0.0150, -0.0220,  0.1854,  0.1628],\n",
       "            [ 0.0198,  0.0943, -0.2356,  0.2286],\n",
       "            [ 0.2566, -0.4351,  0.0296,  0.0034]],\n",
       "  \n",
       "           [[-0.2102,  0.1329,  0.2900, -0.1074],\n",
       "            [ 0.0645,  0.1369, -0.0143,  0.1120],\n",
       "            [ 0.1207, -0.2487,  0.0850, -0.0706],\n",
       "            [-0.2070, -0.3210,  0.1186, -0.3704]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0944,  0.0658, -0.1293, -0.3642],\n",
       "            [-0.0090, -0.2992,  0.2239, -0.2198],\n",
       "            [ 0.0645, -0.1408, -0.0045,  0.1178],\n",
       "            [-0.1453,  0.0040,  0.0797, -0.0439]],\n",
       "  \n",
       "           [[-0.3789, -0.2853,  0.0527,  0.0089],\n",
       "            [-0.2473,  0.3209, -0.0485, -0.0674],\n",
       "            [ 0.1489, -0.0976,  0.2084, -0.3089],\n",
       "            [ 0.2130,  0.1644, -0.2141, -0.0711]],\n",
       "  \n",
       "           [[ 0.1266,  0.1947,  0.2236,  0.1887],\n",
       "            [-0.1034,  0.2325, -0.3273,  0.0444],\n",
       "            [-0.1075,  0.0716,  0.1860,  0.1694],\n",
       "            [-0.1210, -0.1131,  0.0380,  0.0928]]]], requires_grad=True)),\n",
       " ('lin1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0249, -0.0279,  0.0295,  ...,  0.0268,  0.0004, -0.0022],\n",
       "          [-0.0293,  0.0267,  0.0021,  ..., -0.0034,  0.0301, -0.0199],\n",
       "          [-0.0557,  0.0534,  0.0482,  ..., -0.0311,  0.0502, -0.0487],\n",
       "          ...,\n",
       "          [ 0.0096,  0.0215,  0.0387,  ...,  0.0479,  0.0540,  0.0130],\n",
       "          [-0.0584,  0.0030,  0.0391,  ...,  0.0053,  0.0086, -0.0103],\n",
       "          [ 0.0369, -0.0003, -0.0349,  ..., -0.0042,  0.0193, -0.0581]],\n",
       "         requires_grad=True)),\n",
       " ('lin1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         requires_grad=True)),\n",
       " ('lin2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0597,  0.0492, -0.0743,  ..., -0.0053,  0.0045,  0.0180],\n",
       "          [ 0.0362, -0.0202, -0.0413,  ..., -0.0797,  0.0385, -0.0144],\n",
       "          [ 0.0065,  0.0817,  0.0428,  ..., -0.0652, -0.0701, -0.0106],\n",
       "          ...,\n",
       "          [-0.0206,  0.0410, -0.0046,  ..., -0.0348, -0.0173, -0.0052],\n",
       "          [ 0.0570,  0.0288,  0.0053,  ..., -0.0061, -0.0737, -0.0790],\n",
       "          [ 0.0197,  0.0361, -0.0308,  ...,  0.0167, -0.0051,  0.0622]],\n",
       "         requires_grad=True)),\n",
       " ('lin2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.], requires_grad=True))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in torch_model.named_parameters()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
