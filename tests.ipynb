{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379979d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dl import Variable, Module\n",
    "import numpy as np\n",
    "from dl.functions import sum, cross_entropy_loss\n",
    "from dl.modules import Convolution, Linear, ReLU, Flatten, MaxPool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61af5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights_to_torch(model, torch_model):\n",
    "    torch_params = [p for p in torch_model.parameters()]\n",
    "    params = model.parameters()\n",
    "    \n",
    "    assert len(torch_params) == len(params), \"Parameter count mismatch\"\n",
    "\n",
    "    for torch_p, p in zip(torch_params, params):\n",
    "        p = torch.tensor(p.data, dtype=torch.float32)\n",
    "        if p.shape != torch_p.data.shape:\n",
    "            p = p.T\n",
    "\n",
    "        torch_p.data.copy_(p)\n",
    "\n",
    "def compare_grads(model, torch_model):\n",
    "    torch_params = [p for p in torch_model.parameters()]\n",
    "    params = model.parameters()\n",
    "    \n",
    "    assert len(torch_params) == len(params), \"Parameter count mismatch\"\n",
    "\n",
    "    for torch_p, p in zip(torch_params, params):\n",
    "\n",
    "        # My framework stores W in Linear layer as the transpose so that the output is Y = X @ W + b.\n",
    "        # This workaround fails for square weights, so come up with a better solution in the future.\n",
    "        mygrad = p.grad\n",
    "        if p.grad.shape != torch_p.grad.shape:\n",
    "            mygrad = mygrad.T\n",
    "\n",
    "        print(np.allclose(torch_p.grad.numpy(), mygrad, atol=1e-4, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49183ec",
   "metadata": {},
   "source": [
    "## Convolution Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23da9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# def compare_grads(C_in, C_out, K, stride, padding, N_batch, H, W, verbose=False):\n",
    "#     # Input\n",
    "#     X = Variable(np.random.randn(N_batch, C_in, H, W), keep_grad=True)\n",
    "#     X_torch = torch.tensor(X.data, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#     # Your Conv\n",
    "#     conv = Convolution(C_in, C_out, K, stride=stride, padding=padding)\n",
    "#     Y = conv(X)\n",
    "#     z = sum(Y)\n",
    "#     z.backward()\n",
    "\n",
    "#     # PyTorch Conv\n",
    "#     conv_torch = nn.Conv2d(C_in, C_out, K, stride=stride, padding=padding, bias=False)\n",
    "#     W_tensor = torch.tensor(conv.W.data, dtype=torch.float32)\n",
    "#     with torch.no_grad():\n",
    "#         conv_torch.weight.copy_(W_tensor)\n",
    "#     Y_torch = conv_torch(X_torch)\n",
    "#     z_torch = torch.sum(Y_torch)\n",
    "#     z_torch.backward()\n",
    "\n",
    "#     grad_torch_W = conv_torch.weight.grad.numpy()\n",
    "#     grad_mine_W = conv.W.grad()\n",
    "\n",
    "#     grad_torch_X = X_torch.grad.numpy()\n",
    "#     grad_mine_X = X.grad()\n",
    "\n",
    "#     w_close = np.allclose(grad_mine_W, grad_torch_W, atol=1e-4, rtol=1e-3)\n",
    "#     x_close = np.allclose(grad_mine_X, grad_torch_X, atol=1e-4, rtol=1e-3)\n",
    "\n",
    "#     if not (w_close and x_close) and verbose:\n",
    "#         print(f\"FAILED: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={padding}, size=({N_batch},{H},{W})\")\n",
    "#         if not w_close:\n",
    "#             print(\"Weight grad mismatch — max diff:\", np.max(np.abs(grad_mine_W - grad_torch_W)))\n",
    "#         if not x_close:\n",
    "#             print(\"Input grad mismatch — max diff:\", np.max(np.abs(grad_mine_X - grad_torch_X)))\n",
    "\n",
    "#     return w_close and x_close\n",
    "\n",
    "# def run_all_tests():\n",
    "#     paddings = [0, 1, 2, 60]\n",
    "#     strides = [1, 2, 37, 90]\n",
    "#     kernels = [1, 3, 5]\n",
    "#     batch_sizes = [1, 4]\n",
    "#     image_sizes = [(5, 5), (7, 7), (16, 16), (8, 32)]\n",
    "#     channels = [(1, 1), (3, 8), (8, 3), (4, 4)]\n",
    "\n",
    "#     total = 0\n",
    "#     passed = 0\n",
    "\n",
    "#     for pad in paddings:\n",
    "#         for stride in strides:\n",
    "#             for K in kernels:\n",
    "#                 for (C_in, C_out) in channels:\n",
    "#                     for N in batch_sizes:\n",
    "#                         for (H, W) in image_sizes:\n",
    "#                             if H + 2 * pad < K or W + 2 * pad < K:\n",
    "#                                 continue  # Skip invalid kernel size\n",
    "#                             total += 1\n",
    "#                             try:\n",
    "#                                 result = compare_grads(C_in, C_out, K, stride, pad, N, H, W)\n",
    "#                             except Exception as e:\n",
    "#                                 print(f\"\\n❌ Exception for: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={pad}, size=({N},{H},{W})\")\n",
    "#                                 raise e\n",
    "#                             if result:\n",
    "#                                 passed += 1\n",
    "#                             else:\n",
    "#                                 print(f\"\\n❌ Failed for: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={pad}, size=({N},{H},{W})\")\n",
    "#                                 return\n",
    "#     print(f\"✅ Passed {passed} / {total} tests.\")\n",
    "\n",
    "# run_all_tests()\n",
    "\n",
    "# def test_edge_cases():\n",
    "#     assert compare_grads(1, 1, 1, 1, 0, 1, 1, 1)  # Minimal case\n",
    "#     assert compare_grads(3, 4, 3, 2, 4, 16, 7, 7)  # From your original test\n",
    "#     assert compare_grads(8, 16, 5, 1, 2, 2, 32, 32)  # Large channels and image\n",
    "#     assert compare_grads(2, 2, 3, 1, 1, 1, 5, 5)  # Small batch\n",
    "\n",
    "# test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f47751",
   "metadata": {},
   "source": [
    "## Small CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86837d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class MiniCNN(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Convolution(3, 4, 4, 1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool(2, 2)\n",
    "        self.flat1 = Flatten()\n",
    "        self.lin1 = Linear(4*14*14, 783)\n",
    "        self.relu2 = ReLU()\n",
    "        self.lin2 = Linear(783, 100)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = self.conv1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.maxpool1(X)\n",
    "        X = self.flat1(X)\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.lin2(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "class MiniCNNTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 4, 4, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(4*14*14, 783)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(783, 100)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.maxpool1(X)\n",
    "        X = self.flat1(X)\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.lin2(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "N = 16\n",
    "C_in = 3\n",
    "H = 32\n",
    "W = 32\n",
    "\n",
    "X = Variable(np.random.rand(N, C_in, H, W))\n",
    "y = Variable(np.random.randint(0,100,size=(N,)))\n",
    "torch_X = torch.tensor(X.data, dtype=torch.float32)\n",
    "torch_y = torch.tensor(y.data, dtype=torch.int64)\n",
    "\n",
    "model = MiniCNN()\n",
    "torch_model = MiniCNNTorch()\n",
    "\n",
    "transfer_weights_to_torch(model, torch_model)\n",
    "\n",
    "Y = model(X)\n",
    "z = cross_entropy_loss(Y, y)\n",
    "z.backward()\n",
    "\n",
    "torch_Y = torch_model(torch_X) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch_z = criterion(torch_Y, torch_y)\n",
    "torch_z.backward()\n",
    "\n",
    "compare_grads(model, torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580a65f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 0.1854,  0.0110, -0.3825,  0.1056],\n",
       "            [-0.0541,  0.2143,  0.0157,  0.1009],\n",
       "            [-0.0495,  0.3085,  0.1125,  0.0960],\n",
       "            [ 0.1524,  0.0748, -0.3754, -0.0203]],\n",
       "  \n",
       "           [[ 0.0902, -0.2312,  0.0707, -0.1286],\n",
       "            [ 0.3914, -0.1080, -0.2606,  0.3007],\n",
       "            [ 0.1315, -0.1007,  0.0221, -0.1149],\n",
       "            [ 0.0565,  0.0212,  0.3711,  0.1981]],\n",
       "  \n",
       "           [[ 0.3084, -0.0152, -0.1546,  0.1089],\n",
       "            [-0.1067,  0.4666,  0.0616, -0.1474],\n",
       "            [-0.0210,  0.1515,  0.1685,  0.0263],\n",
       "            [ 0.2246,  0.2202, -0.0547, -0.0236]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0235, -0.0415, -0.2790,  0.1413],\n",
       "            [-0.0218, -0.0983, -0.3634, -0.2653],\n",
       "            [ 0.2107,  0.3419,  0.1121, -0.1584],\n",
       "            [ 0.1190,  0.0131, -0.2051,  0.2933]],\n",
       "  \n",
       "           [[ 0.0123,  0.3786,  0.0046, -0.1087],\n",
       "            [ 0.0562,  0.1304,  0.1123,  0.1854],\n",
       "            [-0.0229, -0.1895, -0.2647,  0.0978],\n",
       "            [-0.2268, -0.1455, -0.0281,  0.4187]],\n",
       "  \n",
       "           [[-0.0822, -0.0819, -0.1511,  0.2446],\n",
       "            [ 0.1974, -0.1413, -0.0332,  0.0105],\n",
       "            [ 0.0034, -0.0424, -0.1506,  0.0774],\n",
       "            [-0.4537,  0.2689,  0.0638, -0.0337]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1543, -0.0651,  0.2432, -0.0938],\n",
       "            [-0.0720, -0.3382, -0.1172, -0.2023],\n",
       "            [-0.0209,  0.2178,  0.0234, -0.2091],\n",
       "            [-0.0448, -0.1661, -0.0815,  0.0136]],\n",
       "  \n",
       "           [[ 0.0978,  0.2207, -0.0046,  0.0215],\n",
       "            [ 0.2668,  0.3929,  0.0993, -0.1136],\n",
       "            [-0.2355,  0.1179, -0.1354,  0.3018],\n",
       "            [-0.0302,  0.5274, -0.0043, -0.2961]],\n",
       "  \n",
       "           [[ 0.0852, -0.0095, -0.0562, -0.0378],\n",
       "            [ 0.0938, -0.0407, -0.1278, -0.0877],\n",
       "            [ 0.0513, -0.2118,  0.0430,  0.2381],\n",
       "            [ 0.0422, -0.1785, -0.3991,  0.1393]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0710, -0.0908,  0.0195, -0.3159],\n",
       "            [ 0.0483,  0.2323, -0.0281, -0.3350],\n",
       "            [-0.1571,  0.2046, -0.0590,  0.1243],\n",
       "            [ 0.2837, -0.4203,  0.0129, -0.2677]],\n",
       "  \n",
       "           [[ 0.0918, -0.0591,  0.1744,  0.0960],\n",
       "            [-0.1264, -0.0156,  0.4484,  0.0394],\n",
       "            [ 0.1012,  0.1131,  0.0696, -0.1838],\n",
       "            [ 0.1040, -0.1562, -0.1205, -0.0172]],\n",
       "  \n",
       "           [[ 0.1380, -0.1012,  0.0991, -0.0599],\n",
       "            [-0.0935, -0.3615,  0.0954, -0.1944],\n",
       "            [ 0.1764, -0.0483, -0.0371,  0.1773],\n",
       "            [ 0.1385, -0.2267,  0.1292,  0.0537]]]], requires_grad=True)),\n",
       " ('lin1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0373,  0.0075, -0.0527,  ...,  0.0617,  0.0440, -0.0545],\n",
       "          [ 0.0420,  0.0524,  0.0160,  ...,  0.0170,  0.0105, -0.0351],\n",
       "          [ 0.0292, -0.0423, -0.0387,  ..., -0.0097, -0.0532, -0.0575],\n",
       "          ...,\n",
       "          [-0.0299,  0.0468,  0.0296,  ..., -0.0168,  0.0084,  0.0011],\n",
       "          [-0.0551,  0.0358, -0.0207,  ...,  0.0268, -0.0376,  0.0020],\n",
       "          [-0.0512, -0.0519, -0.0527,  ..., -0.0137,  0.0156,  0.0579]],\n",
       "         requires_grad=True)),\n",
       " ('lin1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         requires_grad=True)),\n",
       " ('lin2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0643,  0.0027, -0.0375,  ..., -0.0150,  0.0100, -0.0084],\n",
       "          [-0.0546, -0.0822, -0.0097,  ..., -0.0625,  0.0492, -0.0745],\n",
       "          [-0.0633, -0.0253, -0.0206,  ..., -0.0345, -0.0619, -0.0673],\n",
       "          ...,\n",
       "          [-0.0198,  0.0505, -0.0584,  ..., -0.0056, -0.0488,  0.0019],\n",
       "          [ 0.0332,  0.0334,  0.0036,  ...,  0.0480, -0.0377,  0.0526],\n",
       "          [-0.0139, -0.0011,  0.0570,  ...,  0.0554, -0.0006,  0.0608]],\n",
       "         requires_grad=True)),\n",
       " ('lin2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.], requires_grad=True))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in torch_model.named_parameters()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
