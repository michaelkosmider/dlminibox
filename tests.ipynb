{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379979d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dl import Variable, Module\n",
    "import numpy as np\n",
    "from dl.functions import sum, cross_entropy_loss\n",
    "from dl.modules import Convolution, Linear, ReLU, Flatten\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61af5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights_to_torch(model, torch_model):\n",
    "    torch_params = [p for p in torch_model.parameters()]\n",
    "    params = model.parameters()\n",
    "    \n",
    "    assert len(torch_params) == len(params), \"Parameter count mismatch\"\n",
    "\n",
    "    for torch_p, p in zip(torch_params, params):\n",
    "        p = torch.tensor(p.data, dtype=torch.float32)\n",
    "        if p.shape != torch_p.data.shape:\n",
    "            p = p.T\n",
    "\n",
    "        torch_p.data.copy_(p)\n",
    "\n",
    "def compare_grads(model, torch_model):\n",
    "    torch_params = [p for p in torch_model.parameters()]\n",
    "    params = model.parameters()\n",
    "    \n",
    "    assert len(torch_params) == len(params), \"Parameter count mismatch\"\n",
    "\n",
    "    for torch_p, p in zip(torch_params, params):\n",
    "\n",
    "        # My framework stores W in Linear layer as the transpose so that the output is Y = X @ W + b.\n",
    "        mygrad = p.grad()\n",
    "        if p.grad().shape != torch_p.grad.shape:\n",
    "            mygrad = mygrad.T\n",
    "\n",
    "        print(np.allclose(torch_p.grad.numpy(), mygrad, atol=1e-4, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49183ec",
   "metadata": {},
   "source": [
    "## Convolution Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# def compare_grads(C_in, C_out, K, stride, padding, N_batch, H, W, verbose=False):\n",
    "#     # Input\n",
    "#     X = Variable(np.random.randn(N_batch, C_in, H, W), keep_grad=True)\n",
    "#     X_torch = torch.tensor(X.data, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#     # Your Conv\n",
    "#     conv = Convolution(C_in, C_out, K, stride=stride, padding=padding)\n",
    "#     Y = conv(X)\n",
    "#     z = sum(Y)\n",
    "#     z.backward()\n",
    "\n",
    "#     # PyTorch Conv\n",
    "#     conv_torch = nn.Conv2d(C_in, C_out, K, stride=stride, padding=padding, bias=False)\n",
    "#     W_tensor = torch.tensor(conv.W.data, dtype=torch.float32)\n",
    "#     with torch.no_grad():\n",
    "#         conv_torch.weight.copy_(W_tensor)\n",
    "#     Y_torch = conv_torch(X_torch)\n",
    "#     z_torch = torch.sum(Y_torch)\n",
    "#     z_torch.backward()\n",
    "\n",
    "#     grad_torch_W = conv_torch.weight.grad.numpy()\n",
    "#     grad_mine_W = conv.W.grad()\n",
    "\n",
    "#     grad_torch_X = X_torch.grad.numpy()\n",
    "#     grad_mine_X = X.grad()\n",
    "\n",
    "#     w_close = np.allclose(grad_mine_W, grad_torch_W, atol=1e-4, rtol=1e-3)\n",
    "#     x_close = np.allclose(grad_mine_X, grad_torch_X, atol=1e-4, rtol=1e-3)\n",
    "\n",
    "#     if not (w_close and x_close) and verbose:\n",
    "#         print(f\"FAILED: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={padding}, size=({N_batch},{H},{W})\")\n",
    "#         if not w_close:\n",
    "#             print(\"Weight grad mismatch — max diff:\", np.max(np.abs(grad_mine_W - grad_torch_W)))\n",
    "#         if not x_close:\n",
    "#             print(\"Input grad mismatch — max diff:\", np.max(np.abs(grad_mine_X - grad_torch_X)))\n",
    "\n",
    "#     return w_close and x_close\n",
    "\n",
    "# def run_all_tests():\n",
    "#     paddings = [0, 1, 2, 60]\n",
    "#     strides = [1, 2, 37, 90]\n",
    "#     kernels = [1, 3, 5]\n",
    "#     batch_sizes = [1, 4]\n",
    "#     image_sizes = [(5, 5), (7, 7), (16, 16), (8, 32)]\n",
    "#     channels = [(1, 1), (3, 8), (8, 3), (4, 4)]\n",
    "\n",
    "#     total = 0\n",
    "#     passed = 0\n",
    "\n",
    "#     for pad in paddings:\n",
    "#         for stride in strides:\n",
    "#             for K in kernels:\n",
    "#                 for (C_in, C_out) in channels:\n",
    "#                     for N in batch_sizes:\n",
    "#                         for (H, W) in image_sizes:\n",
    "#                             if H + 2 * pad < K or W + 2 * pad < K:\n",
    "#                                 continue  # Skip invalid kernel size\n",
    "#                             total += 1\n",
    "#                             try:\n",
    "#                                 result = compare_grads(C_in, C_out, K, stride, pad, N, H, W)\n",
    "#                             except Exception as e:\n",
    "#                                 print(f\"\\n❌ Exception for: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={pad}, size=({N},{H},{W})\")\n",
    "#                                 raise e\n",
    "#                             if result:\n",
    "#                                 passed += 1\n",
    "#                             else:\n",
    "#                                 print(f\"\\n❌ Failed for: Cin={C_in}, Cout={C_out}, K={K}, stride={stride}, pad={pad}, size=({N},{H},{W})\")\n",
    "#                                 return\n",
    "#     print(f\"✅ Passed {passed} / {total} tests.\")\n",
    "\n",
    "# run_all_tests()\n",
    "\n",
    "# def test_edge_cases():\n",
    "#     assert compare_grads(1, 1, 1, 1, 0, 1, 1, 1)  # Minimal case\n",
    "#     assert compare_grads(3, 4, 3, 2, 4, 16, 7, 7)  # From your original test\n",
    "#     assert compare_grads(8, 16, 5, 1, 2, 2, 32, 32)  # Large channels and image\n",
    "#     assert compare_grads(2, 2, 3, 1, 1, 1, 5, 5)  # Small batch\n",
    "\n",
    "# test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f47751",
   "metadata": {},
   "source": [
    "## Small CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7297d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class MiniCNN(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Convolution(C_in = 3, C_out = 4, K = 4)\n",
    "        self.relu1 = ReLU()\n",
    "        self.flat1 = Flatten()\n",
    "        self.lin1 = Linear(29*29*4, 784)\n",
    "        self.relu2 = ReLU()\n",
    "        self.lin2 = Linear(784, 100)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = self.conv1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.flat1(X)\n",
    "        X = self.lin1(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.lin2(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "class MiniCNNTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=4, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(29 * 29 * 4, 784)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(784, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "N = 16\n",
    "C_in = 3\n",
    "H = 32\n",
    "W = 32\n",
    "\n",
    "X = Variable(np.random.rand(N, C_in, H, W))\n",
    "y = Variable(np.random.randint(0,100,size=(N,)))\n",
    "torch_X = torch.tensor(X.data, dtype=torch.float32)\n",
    "torch_y = torch.tensor(y.data, dtype=torch.int64)\n",
    "\n",
    "model = MiniCNN()\n",
    "torch_model = MiniCNNTorch()\n",
    "\n",
    "transfer_weights_to_torch(model, torch_model)\n",
    "\n",
    "Y = model(X)\n",
    "z = cross_entropy_loss(Y, y)\n",
    "z.backward()\n",
    "\n",
    "torch_Y = torch_model(torch_X) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch_z = criterion(torch_Y, torch_y)\n",
    "torch_z.backward()\n",
    "\n",
    "compare_grads(model, torch_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
